\documentclass[12pt,fleqn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb,amsmath,mathrsfs,amsthm}
\usepackage[russian]{babel}
\usepackage[pdftex]{graphicx}

\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
\usepackage[colorlinks, unicode]{hyperref}
%\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm % высота текста
\textwidth=16cm % ширина текста
\oddsidemargin=0pt % отступ от левого края
\topmargin=-1.5cm % отступ от верхнего края
\parindent=24pt % абзацный отступ
\parskip=0pt % интервал между абзацам
\tolerance=2000 % терпимость к "жидким" строкам
\flushbottom % выравнивание высоты страниц
%\def\baselinestretch{1.5}
\setcounter{secnumdepth}{0}
\renewcommand{\baselinestretch}{1.1}

\newcommand{\norm}{\mathop{\rm norm}\limits}
\newcommand{\real}{\mathbb{R}}

\newcommand{\ex}{\mathbb{E}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\intset}{\mathrm{int}}
\newcommand{\softmax}{\mathop{\rm softmax}\limits}
\newcommand{\lossfunc}{\mathcal{L}'}
\newcommand{\dd}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\nm}{\mathcal{N}}
\newcommand{\sle}{\; \Rightarrow \;}
\newcommand{\indpos}{\mathbf{I}_{d_k}^{+}[i, j]}
\newcommand{\indneg}{\mathbf{I}_{d_k}^{-}[i, j]}

\usepackage{pgfplots}

%opening
\title{Краткий конспект лекции \\ <<Практические аспекты использования метода ближайших соседей>>}
\author{Попов Артём Сергеевич \\ курс <<Практикум на ЭВМ>> ММП ВМК МГУ}
\date{5 октября 2018 г.}

\begin{document}
\maketitle

\section{Метод $k$ ближайших соседей}
Рассмотрим задачу многоклассовой классификации.
Пусть дана обучающая выборка $X = (x_i, y_i)^{l}_{i = 1}$, $x_i \in \mathbb{R}^{d}$, $y_i \in \mathbb{Y} = \{1, \ldots, k\}$. 
Пусть также введена \textit{функция расстояния} $\rho: \mathbb{R}^{d} \times \mathbb{R}^{d} \to [0, \infty)$. 
От функции не требуется соответствие всем аксиомам метрики, достаточно лишь неотрицательности. 
Для каждого объекта $x'$ можно расположить объекты обучающей выборки в порядке неубывания расстояний:
\[
\rho(x', x_{(1)}) \leqslant \rho(x', x_{(2)}) \leqslant \ldots \leqslant \rho(x', x_{(l)})
\]

Метрический алгоритм $k$ ближайших соседей (k nearest neighbours, kNN) относит объект $x'$ к тому классу, представителей которого окажется больше всего среди его $k$ ближайших соседей:
\[
a(x') = \arg \max_{y \in Y} \sum_{i = 1}^{k} \mathbb{I}[y_{(i)} = y]
\]

Рассмотренный алгоритм никак не учитывает степень близости объекта $x'$ к его ближайшим соседям. 
Алгоритм $k$ взвешенных ближайших соседей позволяет учесть расстояния до соседей с помощью введения весов объектов, например так:
\[
a(x') = \arg \max_{y \in Y} \sum_{i = 1}^{k} \frac{1}{\varepsilon + \rho(x', x_{(i)})}\mathbb{I}[y_{(i)} = y]
\]

Часто в качестве метрики используется стандартная евклидова метрика или косинусное расстояние ($\rho(x, x') = 1 - \cos(x, x')$).

\section{Методы поиска ближайших соседей}
Существуют различные алгоритмы поиска ближайших соседей.
Самый простой и универсальный способ --- полный перебор всех объектов обучающей выборки. 

Пусть $X \in \real^{l_1 \times d}$ --- обучающая выборка, $Z \in \real^{l_2 \times d}$ --- тестовая выборка. Подсчитаем количество операций, требующихся для подсчёта квадратов всех попарных евклидовых расстояний между объектами обучающей и тестовой выборки.
Квадрат евклидова расстояния между объектами $x$ и $z$ записывается так:
\[
\rho(x, z) = \sum_{s=1}^{d}(x^s - z^s)^2
\]

Можно вычислить все попарные расстояния за $3dl_1 l_2$ операций: $dl_1 l_2$ вычитаний, $dl_1 l_2$ умножений (возведений в~квадрат) и $d l_1 l_2$ сложений. Число операций можно уменьшить с помощью простого трюка. Раскроем скобки в выражении выше:
\[
\rho(x, z) = \sum_{s=1}^{d}({x^s})^2 + \sum_{s=1}^{d}(z^s)^2 - 2 \sum_{s=1}^{d}x^sz^s
\]


Первые две суммы достаточно вычислить для каждого объекта, а не пересчитывать отдельно по каждой паре объектов, это можно сделать за $2d(l_1 + l_2)$ операций. Для вычисления третьей суммы <<в лоб>> потребуется $2dl_1l_2$ операций. Заметим, что вычисление третьей суммы по всем парам объектов можно записать как матричное произведение матриц $X$ и $Z$, которое за счёт эффективной реализации требует меньше операций (алгоритм Копперсмита --- Винограда, перемножение за $O(n^{2.3727})$).
Таким образом, суммарное число операций оказывается намного меньше чем для~первого способа. 

Для евклидовой метрики можно на этапе обучения строить различные структуры данных, позволяющие упростить сложность поиска ближайших соседей, например kd-деревья. С помощью kd-деревьев можно производить поиск соседа быстрее ($O(\log l)$ для одного объекта), но возникают дополнительные затраты для его построения (возможно построение за~$O(n \log n)$). Также дополнительные затраты возникают при добавлении новых объектов в обучающую выборку.

Существуют приближённые методы поиска ближайших соседей. Одним из таких методов является LSH (locality-sensitive hashing). Его идея заключается в построении такой хэш-функции для объектов выборки, которая с большой вероятностью присваивает одинаковые значения близким объектам и разные значения далёким объектам.

\section*{Эффективный подбор параметров в методе kNN}

Одним из основных параметров метода kNN является число ближайших соседей, которое в процессе обучения приходится подбирать по отложенной выборке или кросс-валидации. В общем случае, чтобы замерить качество алгоритма при $m$ значениях параметра на отложенной выборке, необходимо $m$ раз обучить и применить алгоритм. В случае $k$ ближайших соседей это будет стоить $O(mdl^2)$ операций.

Отсортируем значения параметра, которые мы хотим проверить, по неубыванию:
\[k_1 \leqslant k_2 \leqslant \ldots \leqslant k_m\]

Найдём $k_m$ ближайших соседей в обучающей выборке для объектов отложенной выборки.
Заметим, что подсчёт ближайших соседей --- самая вычислительно затратная часть алгоритма.
Очевидно, что для всех остальных значений параметра не нужно проводить поиск ближайших соседей заново, достаточно выбрать из уже найденных соседей нужное количество. 
Таким образом, стоимость алгоритма подбора параметров была сокращена до $O(dl^2)$ операций.
Предложенная схема легко обобщается на случай оценивания алгоритма по кросс-валидации. 

\section*{Преобразования объектов для улучшения качества}

Рассмотрим два простых способа улучшить качество алгоритма kNN с помощью некоторых элементарных преобразований изображений. 
Пусть у нас есть $m$ элементарных преобразований $\phi_j(x) : \real^d \rightarrow \real^d$, $j \in \{ 1, \ldots, m\}$, введём $\phi_0(x) = x$.


Создадим новую обучающую выборку:

\[ X_{new} =\left( \bigcup_{j = 0}^{m} \left(\phi_j(x_i), y_i\right)_{i=1}^{l} \right) \]

Качество алгоритма, обученного выборке $X_{new}$, при удачно выбранных преобразованиях будет превышать исходное.

Другой способ основан на преобразовании объектов тестовой выборки, а не обучающей.
Для каждого объекта тестовой выборки $x'$ получим множество объектов $\Phi(x') = \left\{\phi_j(x') | j \in \{ 1, \ldots, m\} \right\}$.
Введём <<расстояние>> от множества $\Phi(x')$ до объекта обучающей выборки $x$:
\[ \rho(x, \Phi(x')) = \min_{j} \{ \rho(x, \phi_j(x') | j \in \{ 1, \ldots, m\} \}\]

Вычисление $k$ ближайших соседей для $\Phi(x')$ можно организовать следующим образом:
\begin{enumerate}
    \item Для каждого объекта из $\Phi(x')$ найдём его ближайших соседей из обучающей выборки $X$
    \item Среди всех найденных соседей выберем $k$ объектов с наименьшим расстоянием
\end{enumerate}

По полученному множеству ближайших соседей вычисляется ответ алгоритма. Этот способ более эффективен по памяти чем первый.

\end{document}